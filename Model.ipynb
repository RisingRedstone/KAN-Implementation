{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b35844-d35e-49d4-95dd-f0f9a348aca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Network import KANLayer3 as KANLayer\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab4982f9-ab1e-4c79-b77a-e2ba7f2b6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.Module):\n",
    "    def __init__(self, ParameterLayers, seed = None, name = \"\"):\n",
    "        super().__init__(name=name)\n",
    "        self.KANLayers = []\n",
    "        i = 0\n",
    "        for param in ParameterLayers:\n",
    "            self.KANLayers.append(KANLayer(param[0], param[1], GInterval = param[2], p=param[3], seed = seed, name = \"\"+\"_Layer\"+str(i)))\n",
    "            i+=1\n",
    "\n",
    "    def __call__(self, X):\n",
    "        X = tf.keras.layers.Flatten()(X)\n",
    "        for kan in self.KANLayers:\n",
    "            X = kan(X)\n",
    "        return tf.keras.activations.softmax(X, axis = 1)\n",
    "        #return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ec8e63-4130-47d4-bfa0-f9680445ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = [[784, 30, 4, 3],\n",
    "         [30, 20, 10, 3],\n",
    "         [20, 20, 20, 3], \n",
    "         [20, 10, 30, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9254cadd-bfb9-4775-b130-2d8e1d7cdcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_X, train_Y), (test_X, test_Y)) = tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ae8c25-07e7-41be-b644-ebe2e92f1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_X = []\n",
    "train_epoch_Y = []\n",
    "N = 10000\n",
    "i = 0\n",
    "while((i+N) <= 60000):\n",
    "    a = train_Y[i:(i+N)]\n",
    "    b = np.zeros((a.size, a.max() + 1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    train_epoch_X.append(train_X[i:(i+N)]/255)\n",
    "    train_epoch_Y.append(b)\n",
    "    i+=N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a0d599-3026-4405-88b2-d1ed6395bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(Params, seed = 100, name = \"Mod1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4f482e-3aa3-40fd-aa8b-3d9d07156188",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04330e85-458d-41e9-97bd-4914b85c16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\tAcc:  11.25 %\t0.001895323371887207\n",
      "\n",
      "epoch:  1\tAcc:  10.01 %\t0.0009276450157165528\n",
      "\n",
      "epoch:  2\tAcc:  5.56 %\t0.0004532963275909424\n",
      "\n",
      "epoch:  3\tAcc:  8.04 %\t0.0002769891023635864\n",
      "\n",
      "epoch:  4\t"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "epochs = 20\n",
    "for _ in range(epochs):\n",
    "    for X, Y in zip(train_epoch_X, train_epoch_Y):\n",
    "        print(\"epoch: \",i, end = \"\\t\")\n",
    "        with tf.GradientTape() as tape:\n",
    "            Y_pred = model(X)\n",
    "            mse_loss = loss(Y, Y_pred)\n",
    "\n",
    "        \n",
    "        Y_pred_vals = tf.argmax(Y_pred, axis = 1)\n",
    "        Y_true_vals = tf.argmax(Y, axis = 1)\n",
    "        Acc = tf.math.reduce_sum(tf.where(tf.math.equal(Y_true_vals, Y_pred_vals), 1, 0)) * 100 / N\n",
    "        print(\"Acc: \", Acc.numpy(),\"%\", end = \"\\t\")\n",
    "        \n",
    "        tf.print(np.sum(mse_loss)/N)\n",
    "        dY_dM = tape.gradient(mse_loss, model.trainable_variables)\n",
    "\n",
    "\n",
    "        #Correct Nan Values and set them to 0\n",
    "        grads = []\n",
    "        for grad in dY_dM:\n",
    "            grads.append(tf.where(tf.math.is_nan(grad), tf.zeros_like(grad), grad))\n",
    "            if(tf.reduce_any(tf.math.is_nan(grad) == True)):\n",
    "                print(\"NAN_Exists\")\n",
    "        print()\n",
    "                \n",
    "        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bdadc-a118-4217-8747-5040895cca74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
